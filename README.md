# ðŸ“š API de Citas - ExtracciÃ³n y ExposiciÃ³n de Datos

Autor: **Miguel Alvarado**  
Repositorio: [https://github.com/Alvaradom08/api_citas](https://github.com/Alvaradom08/api_citas)

---

## ðŸŽ¯ Objetivo General

Automatizar la extracciÃ³n de informaciÃ³n del sitio [https://quotes.toscrape.com](https://quotes.toscrape.com), almacenarla en una base de datos relacional y exponerla mediante una API REST con filtros.

---

## ðŸŒ Sitio Objetivo

[https://quotes.toscrape.com](https://quotes.toscrape.com)

---

## ðŸ§± Requisitos Funcionales

### 1. ðŸ” Scraping

- Se extraen: **texto de la cita**, **autor**, y **etiquetas**.
- El scraping se ejecuta desde el script `scraping.py`.

### 2. ðŸ’¾ Persistencia (SQL Server)

- Los datos se almacenan en una base de datos SQL Server.
- Se evita la duplicaciÃ³n de autores, citas y etiquetas.
- Se modela la relaciÃ³n N:N entre citas y etiquetas con SQLAlchemy.

### 3. ðŸ§­ API REST

**Endpoints disponibles:**

- `GET /quotes` â†’ todas las citas
- `GET /quotes?author=...` â†’ filtra por autor
- `GET /quotes?tag=...` â†’ filtra por etiqueta
- `GET /quotes?search=...` â†’ bÃºsqueda libre en el contenido

> Los filtros pueden combinarse. La API responde en formato JSON.

---

## ðŸ’» TecnologÃ­as Usadas

- Python 3
- FastAPI
- SQLAlchemy
- SQL Server
- BeautifulSoup + Requests (scraping)
- Uvicorn (servidor ASGI)
- [pyodbc](https://github.com/mkleehammer/pyodbc) (driver para conexiÃ³n a SQL Server)

---

## âš™ InstalaciÃ³n y EjecuciÃ³n

### 1. Clonar el repositorio

git clone https://github.com/Alvaradom08/api_citas.git
cd api_citas

### 2. Crear entorno virtual (opcional)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

### 3. Instalar dependencias
pip install -r requirements.txt

### 4. Configurar conexiÃ³n a SQL Server
Crea un archivo .env en el directorio raÃ­z con el siguiente contenido:
- DATABASE_URL=mssql+pyodbc://USUARIO:CONTRASEÃ‘A@SERVIDOR/NOMBRE_BD?driver=ODBC+Driver+17+for+SQL+Server

### 5. Ejecutar el scraping (crear DB y llenar datos)
python scraping.py

### 6. Levantar el servidor FastAPI

uvicorn main:app --reload

## Estructura del Proyecto
api_citas/
â”œâ”€â”€ BD.py                # Modelos SQLAlchemy
â”œâ”€â”€ crud.py              # Operaciones DB
â”œâ”€â”€ estructura.py        # Esquemas Pydantic
â”œâ”€â”€ main.py              # FastAPI con filtros
â”œâ”€â”€ scraping.py          # Script de scraping
â”œâ”€â”€ requirements.txt     # Dependencias
â”œâ”€â”€ .env                 # Config opcional

## Comandos Ãºtiles (Linux)
### Activar entorno virtual
source venv/bin/activate

### Ejecutar scraping
python scraping.py

### Iniciar la API
uvicorn main:app --reload
