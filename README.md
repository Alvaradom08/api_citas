# ğŸ“š API de Citas â€” Scraping y API REST

Repositorio oficial: [https://github.com/Alvaradom08/api_citas](https://github.com/Alvaradom08/api_citas)

## âœ¨ DescripciÃ³n

Este proyecto automatiza el scraping del sitio [https://quotes.toscrape.com](https://quotes.toscrape.com), extrae citas con sus autores y etiquetas, las almacena en una base de datos relacional (SQL Server) y expone una API REST para consultarlas con filtros combinables.

---

## ğŸ§± TecnologÃ­as

- ğŸ Python 3.11+
- ğŸ•¸ï¸ BeautifulSoup + requests (Scraping)
- âš¡ FastAPI (API REST)
- ğŸ›¢ï¸ SQL Server (Base de datos relacional)
- ğŸ˜ SQLAlchemy (ORM)
- ğŸ“¦ Uvicorn (Servidor ASGI)
- ğŸ“„ `.env` (para variables sensibles)

---

## ğŸ“‚ Estructura del Proyecto

```
api_citas/
â”‚
â”œâ”€â”€app
    â”œâ”€â”€ BD.py                # Modelos SQLAlchemy
    â”œâ”€â”€ CRUD.py              # Operaciones DB
    â”œâ”€â”€ estructura.py        # Esquemas Pydantic
    â”œâ”€â”€ API_Citas.py         # FastAPI con 
    â”œâ”€â”€ conexcionBD.py       # conexcion Base De datos (SQL Server)
    â””â”€â”€ scraping.py          # Script de scraping
â”œâ”€â”€ .env                     # Credenciales de conexiÃ³n
â”œâ”€â”€ requirements.txt         # Dependencias del proyecto
â”œâ”€â”€ README.md                # Este archivo
â””â”€â”€ SCRIPBDCITAS.sql         # Script para crear tablas en SQL Server
```

---

## âš™ï¸ InstalaciÃ³n y ejecuciÃ³n

### ğŸ§° Requisitos previos

- Python 3.11+
- SQL Server en ejecuciÃ³n local
- Git

---

### 1ï¸âƒ£ Clonar el repositorio

```bash
git clone https://github.com/Alvaradom08/api_citas.git
cd api_citas
```

### 2ï¸âƒ£ Crear entorno virtual e instalar dependencias

```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configurar archivo `.env`

Crea un archivo `.env` en la raÃ­z con tu conexiÃ³n a SQL Server:

```env
SERVER=localhost
DATABASE=CITASBD
USERNAME=tu_usuario
PASSWORD=tu_contraseÃ±a
```

> El archivo ya estÃ¡ incluido como plantilla en el proyecto.

---

### 4ï¸âƒ£ Crear base de datos y tablas

Abre SQL Server Management Studio y ejecuta el script:

```sql
  SCRIPBDCITAS.sql

-- Tablas: autores, tags, citas, citasTag (relaciÃ³n N:M)
...
```

---

### 5ï¸âƒ£ Ejecutar scraping (opciÃ³n manual)

```bash
python scraper.py
```

Esto extraerÃ¡ todas las citas y las guardarÃ¡ en la base de datos, evitando duplicados.

---

### 6ï¸âƒ£ Ejecutar la API

```bash
uvicorn main:app --reload
```

La API estarÃ¡ disponible en:  
ğŸ“ http://127.0.0.1:8000

---

## ğŸš€ Endpoints 

###  scraping
```
GET /scrape
```


### âœ”ï¸ Obtener todas las citas

```
GET /quotes
```

### ğŸ” Filtrar por autor

```
GET /quotes?author=Albert Einstein
```

### ğŸ”– Filtrar por etiqueta

```
GET /quotes?tag=inspirational
```

### ğŸ“ BÃºsqueda libre por contenido

```
GET /quotes?search=life
```

### ğŸ“Œ CombinaciÃ³n de filtros

```
GET /quotes?author=Albert Einstein&tag=inspirational&search=truth
```

---

## ğŸ§ª Ejemplos de uso con `curl`

```bash
curl "http://127.0.0.1:8000/quotes?tag=truth"
curl "http://127.0.0.1:8000/quotes?author=Albert%20Einstein"
curl "http://127.0.0.1:8000/quotes?search=change"
```

---

## ğŸ§ Comandos Ãºtiles en Linux

```bash
# Crear entorno virtual
python3 -m venv venv
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar scraping
python scraper.py

# Iniciar API
uvicorn main:app --reload
```



## ğŸ‘¤ Autor

** Juan Mateo Alvarado Montoya**  
[https://github.com/Alvaradom08](https://github.com/Alvaradom08)

---

## ğŸ“„ Licencia

MIT License â€“ Libre uso acadÃ©mico o personal.

---